---
title: "Practical Machine Learning Course Project"
author: "Nsi J."
date: "7 juillet 2017"
output: 
  html_document: 
    keep_md: yes
---

# Executive summary   

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement. The data come from  see <http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises>    

# The goal of the project     

One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, the goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants". They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.    

# Set the work directory

```{r Set the work directory, echo=FALSE}
rm(list=ls()) 
WD <- getwd()
if (!is.null(WD)) setwd(WD)
```

# Load data and perform basic exploratory data analysis
```{r Load data}
Trainurl <- 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'
Testurl  <- 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv'
if (!file.exists("data")) dir.create("data")
if (!file.exists("pml-training.csv")) download.file(Trainurl, destfile1 ="data/pml-training.csv", mode="wb")
if (!file.exists("pml-testing.csv")) download.file(Testurl, destfile2 ="data/pml-testing.csv", mode="wb")
library(dplyr)
library(data.table)
training <-read.csv("pml-training.csv",header=TRUE, sep=",", na.strings=c("NA","", "#DIV/0!"))
testingVal <-read.csv("pml-testing.csv",header=TRUE, sep=",", na.strings = c("NA","", "#DIV/0!"))
```
```{r  basic exploratory data analysis}
# Structure of the dataset
# str(training)
## 'data.frame':	19622 obs. of  160 variables
#dim(training)
#[1] 19622   160
# View(training)
## Many rows with NA data,#DIV/0! values,"" 
which(colnames(training)=="classe")
(table(training$classe))
table(training$user_name)
table(training$user_name, training$classe)

# Classe A has the highest value for the 6 participants
plot(training$user_name, training$classe, xlab="Classe activity by participant")

#str(testingVal)
## data.frame':	20 obs. of  160 variables
# names(testingVal)
# which(colnames(testingVal)=="classe")
# Testing does not contain classe named column
# View(testingVal)

```
```{r  basic Preprocessing to handle NA data on training and testing data set}
# Math functions as mean, sd, max, colSums...all take the na.rm argument, TRUE ie missing values are omitted
# see <http://faculty.nps.edu/sebuttre/home/R/missings.html>

training <- training[, colSums(is.na(training)) == 0]
#dim(training)
# [1] 19622    60
# New data set with only 19622 observations and 60 variables instead of 19622 observations and 160 variables

testingVal <- testingVal[, colSums(is.na(testingVal)) == 0]

#dim(testingVal)
# [1] 20 60
# New data set with only 20 observations and 60 variables instead of 20 observations and 160 variables

```
# Steps by Steps to predict the manner in which the participants did the exercise   

```{r Analysis steps, test and validation}
library(caret)
set.seed(2433)

# CROSS VALIDATION : how to detect relevant features and build models 
# DATA SPLITTING
inTrain <- createDataPartition(training$classe, p=0.70, list=F)
training <- training[inTrain, ]
testing <- training[-inTrain, ]
# dim(training)
## [1] 13737    60
# View(training)
# which(colnames(training)=="classe")
## [1] 60
# dim(testing)
## [1] 4141   60

# The effect of all the variables on the outcome classe with the choice of the method  randomforest as it knowns to be more accuracy. Furthermore, it's usually one of the two top performing algorithms along with boosting in prediction contests
library(randomForest)
library(mlbench)
library(caret)
library(parallel)
library(doParallel)

cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
fitControl <- trainControl(method = "cv", number = 10, allowParallel = TRUE)
library(foreach)
library(iterators)

# FIT A MODEL 
set.seed(3135)
modelRf <- train(classe ~ ., method="rf",data=training,trControl = fitControl)
modelRf
## Random Forest 

## 13737 samples
##    59 predictor
##    5 classes: 'A', 'B', 'C', 'D', 'E' 

##No pre-processing
##Resampling: Cross-Validated (10 fold) 
##Summary of sample sizes: 12364, 12363, 12364, 12364, 12363, 12362, ... 
##Resampling results across tuning parameters:

##  mtry  Accuracy   Kappa    
##  2     0.9948307  0.9934613
##  41    0.9998544  0.9998158
##  81    0.9998544  0.9998158

##Accuracy was used to select the optimal model using  the largest value.
##The final value used for the model was mtry = 41.

stopCluster(cluster)
registerDoSEQ()

# ACCURACY of the model RandomForest :Steps to evaluate the suitability of this model

pred_rf <- predict(modelRf, training)
# confusionMatrix(pred_rf, training$classe)
##Confusion Matrix and Statistics

##          Reference
##Prediction    A    B    C    D    E
##         A 3906    0    0    0    0
##         B    0 2658    0    0    0
##         C    0    0 2396    0    0
##         D    0    0    0 2252    0
##         E    0    0    0    0 2525

##Overall Statistics
                                     
##               Accuracy : 1          
##                 95% CI : (0.9997, 1)
##    No Information Rate : 0.2843     
##    P-Value [Acc > NIR] : < 2.2e-16  
                                     
##                  Kappa : 1          
## Mcnemar's Test P-Value : NA         

##Statistics by Class:

##                     Class: A Class: B Class: C Class: D Class: E
##Sensitivity            1.0000   1.0000   1.0000   1.0000   1.0000
##Specificity            1.0000   1.0000   1.0000   1.0000   1.0000
##Pos Pred Value         1.0000   1.0000   1.0000   1.0000   1.0000
##Neg Pred Value         1.0000   1.0000   1.0000   1.0000   1.0000
##Prevalence             0.2843   0.1935   0.1744   0.1639   0.1838
##Detection Rate         0.2843   0.1935   0.1744   0.1639   0.1838
##Detection Prevalence   0.2843   0.1935   0.1744   0.1639   0.1838
##Balanced Accuracy      1.0000   1.0000   1.0000   1.0000   1.0000

# RESULT : The method RandomForest has an accurary nearly about 100% on the confidence interval of 95%

# THE FINAL MODEL
# Adding some parameters to see the out of bag (OOB) error which is the overall classification error during training, by both class and ntree
modelfinalRf <- train(classe ~ ., method="rf",data=training,trControl = fitControl, importance=TRUE, ntree=50, do.trace=F)

pred_RF <- predict(modelfinalRf, training)
confusionMatrix(pred_RF, training$classe)

# Result with adding parameters : The method RandomForest still has an accuracy nearly about 100% on the confidence interval of 95%

# PREDICTION ON TESTING DATA SET : Accuracy of the model on testing data set? 

pred_testRF <- predict(modelfinalRf, testing)
#confusionMatrix(pred_testRF, testing$classe)
##Confusion Matrix and Statistics

##          Reference
##Prediction    A    B    C    D    E
##         A 1165    0    0    0    0
##         B    0  797    0    0    0
##         C    0    0  719    0    0
##         D    0    0    0  693    0
##         E    0    0    0    0  737

##Overall Statistics
                                     
##               Accuracy : 1          
##                 95% CI : (0.9991, 1)
##    No Information Rate : 0.2834     
##    P-Value [Acc > NIR] : < 2.2e-16  
                                     
##                  Kappa : 1          
## Mcnemar's Test P-Value : NA         

##Statistics by Class:

##                     Class: A Class: B Class: C Class: D Class: E
##Sensitivity            1.0000   1.0000   1.0000   1.0000   1.0000
##Specificity            1.0000   1.0000   1.0000   1.0000   1.0000
##Pos Pred Value         1.0000   1.0000   1.0000   1.0000   1.0000
##Neg Pred Value         1.0000   1.0000   1.0000   1.0000   1.0000
##Prevalence             0.2834   0.1939   0.1749   0.1686   0.1793
##Detection Rate         0.2834   0.1939   0.1749   0.1686   0.1793
##Detection Prevalence   0.2834   0.1939   0.1749   0.1686   0.1793
##Balanced Accuracy      1.0000   1.0000   1.0000   1.0000   1.0000

# The method RandomForest has an accurary nearly about 100% (99.91%)on the confidence interval of 95% though a little bit less than the one on training data set(99.97%)

# The expected OUT OF SAMPLE ERROR 
# It is supposed to be larger than in sample error which you get on the same data set used to build your predictors
# From the confusionMatrix, we have the value of accuracy. Accuracies are also computed as accuracies = 1 - errors => errors = 1 - accuracies
#see < https://stackoverflow.com/questions/28666066/get-the-accuracy-of-a-random-forest-in-r)>

# Err <- 1 - 0.9991)
## Err
## [1] 9e-04
## The err is about 0.09%

library(ggplot2)

qplot(pred_testRF, color=classe, data=testing, main='Testing data Prediction')


# VALIDATION PART: Apply the machine learning algorithm to the 20 test cases 


validation <-testingVal


pred_validRF <- predict(modelfinalRf,validation)
#pred_validRF
##  [1] A A A A A A A A A A A A A A A A A A A A
## Levels: A B C D E


# Evaluate on Validation
# names(validation)

pred_validRF <- as.numeric(pred_validRF)
# sqrt(sum(pred_validRF-60)^2)  --minus the column named classe (training data set)
## [1] 1180
```    

# Conclusion   
After exploring data, then handling missing values the data has been splitted into training/testing, respectively 70/30. The choice of randomforest method to fit a model seems to be the best on this context of predictions with an accuracy close to 100% with 60 variables, with an error less than 0 on testing data. Howerver, the drawback might be the overfitting of all of the 60 variables in this model.

